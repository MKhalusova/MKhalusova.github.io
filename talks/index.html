---
layout: default
title: Maria Khalusova
---
<div class="blurb">
    <h1>Upcoming talks</h1>

    <h2>Debugging Jupyter Notebooks</h2>
    <p><strong>When</strong>: November 16th, 2019</p>
    <p><strong>Where</strong>: <a href="https://2019.pycon.ca/talks/talk-44/">PyCon, Canada</a></p>
    <p><strong>Description</strong>: Writing code in a Jupyter Notebook is an interactive process involving a lot of
        trial and error. As your code evolves, errors and bugs inevitably start to creep in. A debugger can help track
        them down. In this talk, we’ll go through some reasons why you may want to debug your notebook. Then, we’ll
        explore how you can debug notebooks with the ipdb debugger. Finally, we’ll see how we can use an IDE to track
        down those pesky bugs. </p>

    <h2>Machine Learning Model Evaluation Metrics</h2>
    <p><strong>When</strong>: December 5th, 2019</p>
    <p><strong>Where</strong>: <a
            href="https://pydata.org/la2019/schedule/presentation/1/machine-learning-model-evaluation-metrics/">PyData
        LA</a></p>
    <p><strong>Description</strong>: Choosing the right evaluation metric for your machine learning project is crucial,
        as it decides which model you’ll ultimately use. How do you choose an appropriate metric? This talk will explore
        the important evaluation metrics used in regression and classification tasks, their pros and cons, and how to
        make a smart decision.</p>

    <h2>Getting Started with Apache Spark</h2>
    <p><strong>When</strong>: January 10th, 2019</p>
    <p><strong>Where</strong>: <a href="https://www.dawsoncollege.qc.ca/dawscon/">DawsCon, Montreal</a></p>
    <p><strong>Description</strong>: Since its first release in 2014, Apache Spark™ has gain massive popularity and has
        become the standard for large-scale distributed data processing. Not only it has been rapidly adopted by
        companies across a wide range of industries, it has quickly become the largest open source community in big
        data, with over 1000 contributors.

        In this talk we’ll introduce the basics of Spark architecture, core concepts and available APIs. Then, we’ll
        take a look at some examples of what you can do with Apache Spark.</p>

    <h2>A closer look: Exploratory Data Analysis with Spark</h2>
    <p><strong>When</strong>: February 26-28, 2020</p>
    <p><strong>Where</strong>: <a
            href="https://confoo.ca/en/yul2020/session/a-closer-look-exploratory-data-analysis-with-spark">ConFoo,
        Montreal</a></p>

    <h2>Machine Learning Model Evaluation Metrics</h2>
    <p><strong>When</strong>: February 26-28, 2020</p>
    <p><strong>Where</strong>: <a href="https://confoo.ca/en/yul2020/session/machine-learning-model-evaluation-metrics">ConFoo,
        Montreal</a></p>

    <h1>Past talks</h1>

    <h2>Seeking inspiration in data through Exploratory Data Analysis with Apache Spark</h2>
    <p><strong>Where</strong>: Big Data World, Singapore</p>
    <p><strong>Description</strong>: There are various reasons to perform Exploratory Data Analysis. Sometimes you need
        to prepare the data for a machine learning algorithm, other times you have a specific question you want to get
        answered by visualizing the data. It can be that you've got all that data, and you're not sure yet how it can be
        useful. Data Science has a lot of room for creativity. In this talk, we'll take a dataset, explore it with Spark
        and IntelliJ IDEA, and see if we can get inspired by it.</p>

    <h2>The missing piece in the Data Engineer's toolset</h2>
    <p><strong>Where</strong>: Big Data World, Singapore</p>
    <p><strong>Description</strong>: The success of any data-driven solution relies on accessible, pre-processed data.
        Raw data never comes in polished .csv files ready for machine learning, it's always a mess. It needs to be
        collected, stored, organised, cleaned, pre-processed, made available for consumption.

        This is done by Data Engineers who write code to build data infrastructure, to transform the data, to configure
        the pipelines, to create custom tools specific for their company's needs.

        And yet the Notebooks used to write that code provide close to no assistance. A typo in a method name could
        results in hours of work wasted - waiting for that Spark job to finish only to crash on the last line. It
        doesn't have to be that way. At JetBrains, we have vast experience in intelligent coding assistance, and we're
        ready to bring this experience to the Spark world.</p>

    <h2>Machine Learning Model Evaluation Metrics</h2>
    <p><strong>Where</strong>: MusicCityTech Data, Nashville, TN</p>
    <p><strong>Description</strong>: Choosing the right evaluation metric for your machine learning project is crucial,
        as it decides which model you’ll ultimately use. Those coming to ML from software development are often
        self-taught, but practice exercises and competitions generally dictate the evaluation metric. In a real-world
        scenario, how do you choose an appropriate metric? This talk will explore the important evaluation metrics used
        in regression and classification tasks, their pros and cons, and how to make a smart decision</p>

    <h2>A closer look: Exploratory Data Analysis with Spark and IntelliJ IDEA</h2>
    <p><strong>Where</strong>: Data Con LA, Los Angeles, CA</p>
    <p><strong>Description</strong>: A typical workflow of a Data Scientist involves some level of exploratory data
        analysis. If you’re using Python when working with your data, you are probably quite familiar with packages like
        pandas, matplotlib, seaborn and others that help you get the initial familiarity with the data and understand
        what are the best approaches for your next step. Switching from pandas to Spark - how do you explore your data?
        How do you visualise it? How do you understand it better before crafting your Spark jobs?

        In this talk I’ll take a dataset and will guide you through the numerous ways you can explore your data with
        Spark and a new plugin for IntelliJ IDEA.</p>
    <p><strong><a
            href="https://speakerdeck.com/mkhalusova/a-closer-look-exploratory-data-analysis-with-spark-and-intellij-idea">Slides</a></strong>
    </p>
    <p><strong><a href="https://github.com/MKhalusova/DataConDemo">Demo code</a></strong></p>

    <h2>Machine Learning Model Evaluation Metrics</h2>
    <p><strong>Where</strong>: ML4ALL, Portland, OR</p>
    <p><strong>Description</strong>: Choosing the right evaluation metric for your machine learning project is crucial,
        as it decides which model you’ll ultimately use. Those coming to ML from software development are often
        self-taught, but practice exercises and competitions generally dictate the evaluation metric. In a real-world
        scenario, how do you choose an appropriate metric? This talk will explore the important evaluation metrics used
        in regression and classification tasks, their pros and cons, and how to make a smart decision.</p>
    <p><strong><a href="https://www.youtube.com/watch?v=F6gpHfx3BCs&t=2s">Recording</a></strong></p>

    <h2>Machine Learning Model Evaluation Metrics</h2>
    <p><strong>Where</strong>: AnacondaCON 2019, Austin, TX</p>
    <p><strong>Description</strong>: Choosing the right evaluation metric for your machine learning project is crucial,
        as it decides which model you’ll ultimately use. Those coming to ML from software development are often
        self-taught, but practice exercises and competitions generally dictate the evaluation metric. In a real-world
        scenario, how do you choose an appropriate metric? This talk will explore the important evaluation metrics used
        in regression and classification tasks, their pros and cons, and how to make a smart decision.</p>
    <p><strong><a href="https://www.youtube.com/watch?v=wpQiEHYkBys&t=540s">Recording</a></strong></p>

    <h2>Visual Version Control with JetBrains IDEs</h2>
    <p><strong>Where</strong>: CodeMash 2019, Sandusky, OH</p>
    <p><strong>Note</strong>: Co-presented with Paul Everitt.</p>
    <p><strong>Description</strong>: Using version control systems (VCS) like git from the command-line is powerful, but
        arcane and error-prone. Moreso for more-advanced features. JetBrains IDEs put a visual face on version control,
        making you more productive and tying version control into the rest of your programming workflow. In this
        session, we show “Visual VCS” by simulating a real-world collaboration between two developers on a project.
        We’ll cover cloning repositories, branching, committing changes, merging, creating pull requests, and other
        aspects of the GitHub Flow model. We’ll finish with an overview of more-powerful VCS features in the IDE.</p>

    <h2>Code Review Matters and Manners</h2>
    <p><strong>Where</strong>: JavaZone 2016, Norway</p>
    <p><strong>Note</strong>: Co-presented with Trisha Gee.</p>
    <p><strong>Description</strong>: A code review is basically a technical discussion which should lead to improvements
        in the code and/or sharing knowledge in a team. As with any conversation, it should have substance and form.

        What's involved in a good code review? What kind of problems do we want to spot and address? Trisha Gee will
        talk about things a reviewer may consider when looking at changes: what potential issues to look for; why
        certain patterns may be harmful; and, of course, what NOT to look at.

        But when it comes to commenting on someone's work, it may be hard to find the right words to convey a useful
        message without offending the authors - after all, this is something that they worked hard on. Maria Khalusova
        will share some observations, thoughts and practical tricks on how to give and receive feedback without turning
        a code review into a battlefield.</p>
    <p><strong><a href="https://vimeo.com/182087729">Recording</a></strong></p>

    <h2>How to stop wasting your time and start performing useful code reviews</h2>
    <p><strong>Where</strong>: Devoxx France 2016</p>
    <p><strong>Description</strong>: Many teams are adopting code review now, and then find themselves struggling to
        make it work. Do you feel that it takes too much time? Do you think the issues that reviewers find are not
        really deal-breakers? Are you afraid that instead of improved collaboration you are creating a battlefield over
        the code base? There are a number of things that you can do to make code reviews go smoothly and bring
        meaningful, useful results. We'll talk about how we can make code reviews transparent and efficient. We'll
        outline what things we should be looking for when reviewing someone else's code, and how not to lose focus
        hunting extra spaces. And more importantly, we'll talk about the human factor, and how while it's not always
        possible to separate a code from its authors, there are ways to communicate messages effectively.</p>
    <p><strong><a href="https://www.youtube.com/watch?v=-tcy4z0hszg">Recording</a></strong></p>

    <h2>How to get the most out of code reviews</h2>
    <p><strong>Where</strong>: JavaDay Kiev 2015</p>
    <p><strong>Description</strong>: As developers we always look for ways to do things faster, better, and automate as
        much as possible. We write code in top-notch IDEs that have static code analysis, automatic refactoring and so
        on, we run unit tests, we use CI servers, and issue trackers, we adopt agile practices to get feedback and
        deliver as fast as possible. As far as code review practice goes, there's still a lot to be improved, and in
        this talk I'm going to tell you how you can perform efficient, transparent and useful code reviews.</p>
    <p><strong><a
            href="https://www.slideshare.net/JavaDayUA/how-to-get-the-most-out-of-code-reviews">Slides</a></strong></p>

</div>